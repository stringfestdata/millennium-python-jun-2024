{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "This notebook will explain the following topics and concepts:\n",
    "\n",
    "**Concatenation**\n",
    "- Rows and Columns\n",
    "\n",
    "**Merging Data**\n",
    "- left\n",
    "- right\n",
    "- inner\n",
    "- outer\n",
    "\n",
    "**Joining Data**\n",
    "- left\n",
    "- right\n",
    "- inner\n",
    "- outer\n",
    "\n",
    "**Grouping Data**\n",
    "- by time\n",
    "- by columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries & Load Data\n",
    "\n",
    "We'll use the same csv files as we used in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# format for floats\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GOOGL = pd.read_csv(filepath_or_buffer='../Data/GOOGL.csv', index_col='Date', parse_dates=True)\n",
    "df_IBM = pd.read_csv(filepath_or_buffer='../Data/IBM.csv', index_col='Date', parse_dates=True)\n",
    "df_MSFT = pd.read_csv(filepath_or_buffer='../Data/MSFT.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation\n",
    "\n",
    "- Glues together DataFrames, without much intelligence.\n",
    "- Dimensions should match along the axis you are concatenating on. \n",
    "- Use **pd.concat** and pass in a list of DataFrames to concatenate together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a few simple DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range(start='2017', freq='BQ', periods=4)\n",
    "cols  = ['Open', 'Close']\n",
    "\n",
    "df1 = df_IBM.reindex(date_range)[cols]\n",
    "df2 = df_GOOGL.reindex(date_range)[cols]\n",
    "df3 = df_MSFT.reindex(date_range)[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate\n",
    "\n",
    "The default is to concatenate the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, df2, df3])\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the columns\n",
    "Pass the `axis = 1` parameter to pd.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, df2, df3], axis=1)\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging\n",
    "\n",
    "Pandas has two important functions for joining DataFrames together which intelligently try to align values from selected columns of each DataFrame. These functions are called **merge** and **join**. These functions use a similar logic to joins in SQL.\n",
    "\n",
    "First we will look at merge.\n",
    "\n",
    "**There are 4 Different types of merge**\n",
    "- **Inner Merge** – The default Pandas behaviour, only keep rows where the merge “on” value exists in both the left and right DataFrames.\n",
    "- **Left Merge** – (aka left merge or left join) Keep every row in the left DataFrame. Where there are missing values of the “on” variable in the right DataFrame, add empty / NaN values in the result.\n",
    "- **Right Merge** – (aka right merge or right join) Keep every row in the right DataFrame. Where there are missing values of the “on” variable in the left column, add empty / NaN values in the result.\n",
    "- **Outer Merge** – A full outer join returns all the rows from the left DataFrame, all the rows from the right DataFrame, and matches up rows where possible, with NaNs elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some sample DataFrames\n",
    "\n",
    "Just a few days worth of Data from Google and IBM\n",
    "\n",
    "Note the difference in date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['High', 'Low']\n",
    "\n",
    "df1 = df_IBM[cols]['2017-Jan-01':'2017-Jan-06'].sort_index()\n",
    "df2 = df_GOOGL[cols]['2017-Jan-05':'2017-Jan-10'].sort_index()\n",
    "\n",
    "# show both dataframes\n",
    "print(\"== IBM ==\")\n",
    "display(df1)\n",
    "print(\"== GOOGLE ==\")\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Merge\n",
    "\n",
    "Only keep values for Dates found in both left (df1) and right (df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of merge works in latest Pandas\n",
    "pd.merge(df1, df2, how='inner', on='Date')\n",
    "\n",
    "# This version of merge works in older Pandas\n",
    "pd.merge(df1, df2, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Merge\n",
    "\n",
    "- Keep everything in the left DataFrame.\n",
    "- Where nothing exists in the right DataFrame, fill with NaN (\"Not a Number\" - these are empty values).\n",
    "- Use the suffixes parameter to override the x_ and y_ defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcs = ['_IBM','_GOOGL']\n",
    "\n",
    "pd.merge(df1, df2, how='left', on='Date', suffixes=srcs)\n",
    "\n",
    "pd.merge(df1, df2, how='left', left_index=True, right_index=True,  suffixes=srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right Merge\n",
    "\n",
    "- Keep everything in the right DataFrame\n",
    "- Where nothing exists in the left DataFrame, fill with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='right', on='Date', suffixes=srcs)\n",
    "\n",
    "pd.merge(df1, df2, how='right', left_index=True, right_index=True, suffixes=srcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Merge\n",
    "\n",
    "Keep everything in both left and right DataFrames, fill with NaN where no data present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer', on='Date', suffixes=srcs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining\n",
    "\n",
    "- The second pandas function for intelligently combining DataFrames is called **join**.\n",
    "- Join is **very** similar to merge.\n",
    "- As with merge, the **how** parameter takes inner, outer, left or right.\n",
    "- As with merge, the **on** parameter is the name of a column to join on.\n",
    "- However there is one major difference:\n",
    "  - When using join the \"on\" **must** be the index in at least one of the DataFrames.\n",
    "  - Merge will allow the \"on\" to be a regular column in **both** DataFrames.\n",
    "\n",
    "The syntax for calling the two functions is also slightly different:\n",
    "- **join**  : df1.join(df2, how=\"inner\", on=\"Date\")\n",
    "- **merge** : pd.merge(df1, df2, how=\"inner\", on=\"Date\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = ['High', 'Low']\n",
    "cols2 = ['Open', 'Close']\n",
    "df1 = df_IBM[cols1]['2017-Jan-01':'2017-Jan-07'].sort_index()\n",
    "df2 = df_IBM[cols2]['2017-Jan-04':'2017-Jan-11'].sort_index()\n",
    "\n",
    "# show both dataframes\n",
    "print(\"== High & Low ==\")\n",
    "display(df1)\n",
    "print(\"== Open & Close ==\")\n",
    "display(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't specify an \"on\" then we will join \"on\" the index of both DataFrames\n",
    "df1.join(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, how='outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Data\n",
    "\n",
    "Pandas provides functions that allow us to group rows of data together and call aggregate functions on them as a unit e.g. mean, max, min, std, etc.\n",
    "\n",
    "To create a group we call the **groupby** method on a DataFrame.\n",
    "\n",
    "e.g. Create groups where the 'Industry' column is the same:\n",
    "    - df1.groupby('Industry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Columns\n",
    "\n",
    "Use the **by** parameter and supply a column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath_or_buffer='../Data/groups.csv',index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping is a convenient way to get the mean for each sector of each column\n",
    "df.groupby(by='Sector').mean(numeric_only=True)\n",
    "\n",
    "# Grouping is a convenient way to get the mean for each sector of each column\n",
    "df.groupby(by='Rep').mean(numeric_only=True)\n",
    "\n",
    "# Grouping is a convenient way to get the mean for each sector of each column\n",
    "df.groupby(by='Portfolio').mean(numeric_only=True)\n",
    "\n",
    "# Or we could create a variable to store the name of the function\n",
    "func = 'std'\n",
    "df.groupby(by='Sector').agg(func,numeric_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Time Period\n",
    "Often we want to group according to a frequency e.g. a group of all values in a single business quarter.\n",
    "\n",
    "We can then call mean for all of the groups, e.g. get the mean Volume per business quarter.\n",
    "\n",
    "A convenient way to group by a frequency is to use the Grouper object (in the pandas namespace).\n",
    "\n",
    "- pd.Grouper\n",
    "- Most commonly used to pass in a frequency\n",
    "- Group by frequencies: B (business day), BQ (business quarter), M (month), Y (year), etc.\n",
    "- It's also possible to group by specific time frequencies e.g. 15d, 1h30min etc.\n",
    "  - See the list of frequency aliases: http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.Grouper(freq='BQ')).mean(numeric_only=True)\n",
    "\n",
    "# Or we could create a variable to store the grouper\n",
    "by_BMonth = pd.Grouper(freq='BM')\n",
    "df.groupby(by=by_BMonth).mean(numeric_only=True)\n",
    "\n",
    "# Or use teh agg and a list of functions\n",
    "funcs = 'mean'\n",
    "df.groupby(by=by_BMonth).agg(funcs, numeric_only=True)\n",
    "\n",
    "\n",
    "# Finally we can supply a list of groupers\n",
    "by_BMonth = pd.Grouper(freq='BM')\n",
    "by_Rep = pd.Grouper(key='Rep')\n",
    "by_Sector = pd.Grouper(key='Sector')\n",
    "\n",
    "# And perform a variety of slice and dice operations\n",
    "groups1 = [by_Rep, by_BMonth, by_Sector]\n",
    "df.groupby(by=groups1).agg(funcs, numeric_only=True)\n",
    "\n",
    "groups2 = [by_Sector, by_Rep, by_BMonth]\n",
    "df.groupby(by=groups2).agg(funcs, numeric_only=True)\n",
    "\n",
    "groups3 = [by_BMonth, by_Sector, by_Rep]\n",
    "df.groupby(by=groups3).agg(funcs, numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "752a8dbf0d2606d0e4a8fd11745a0f3cf206e4848a02eb7bb5eda5c098d7f642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
